{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a94afe4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/octoopt/workspace/projects/learn-from-basics/nlp-vietnamese-phd/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pymupdf\n",
    "import xml.etree.ElementTree as ET\n",
    "from xml.dom import minidom\n",
    "import re\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "\n",
    "from openai import OpenAI\n",
    "from underthesea import ner\n",
    "\n",
    "# ──────────────── CẤU HÌNH ────────────────\n",
    "BOOK_METADATA = {\n",
    "    \"NAM_HOA_KINH\": {\n",
    "        \"TITLE\": \"Nam Hoa Kinh\",\n",
    "        \"VOLUME\": \"\",\n",
    "        \"AUTHOR\": \"Trang Tử\",\n",
    "        \"PERIOD\": \"Chiến Quốc\",\n",
    "        \"LANGUAGE\": \"vi\",\n",
    "        \"SOURCE\": \"thuviensach.vn\",\n",
    "    },\n",
    "    \"TRANG_TU_NAM_HOA_KINH\": {\n",
    "        \"TITLE\": \"Nam Hoa Kinh (Bản học thuật)\",\n",
    "        \"VOLUME\": \"\",\n",
    "        \"AUTHOR\": \"Trang Tử\",\n",
    "        \"PERIOD\": \"Chiến Quốc\",\n",
    "        \"LANGUAGE\": \"vi\",\n",
    "        \"SOURCE\": \"Bản học thuật số hoá\",\n",
    "    },\n",
    "    \"TRANG_TU_NAM_HOA_KINH_2\": {\n",
    "        \"TITLE\": \"Nam Hoa Kinh (Bản dịch)\",\n",
    "        \"VOLUME\": \"\",\n",
    "        \"AUTHOR\": \"Trang Tử\",\n",
    "        \"PERIOD\": \"Chiến Quốc\",\n",
    "        \"LANGUAGE\": \"vi\",\n",
    "        \"SOURCE\": \"Bản dịch hiện đại\",\n",
    "    },\n",
    "}\n",
    "\n",
    "BASED_ENTITY_GROUPS = [\n",
    "    \"PER\",\n",
    "    \"ORG\",\n",
    "    \"LOC\",\n",
    "    \"ORG\",\n",
    "    \"TME\",\n",
    "    \"TITLE\",\n",
    "    \"NUM\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d5ed4246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────── HÀM TIỆN ÍCH ────────────────\n",
    "\n",
    "def normalize(s: str) -> str:\n",
    "    return unicodedata.normalize(\"NFC\", s.strip())\n",
    "\n",
    "\n",
    "def clean_text(text: str, clean_patterns: list[str] = None) -> str:\n",
    "    \"\"\"\n",
    "    Làm sạch văn bản khỏi các ký tự HTML, dấu ngoặc, URL, và mẫu đặc biệt.\n",
    "    \"\"\"\n",
    "\n",
    "    if clean_patterns is None:\n",
    "        clean_patterns = [\"***\", \"---\", \"___\"]\n",
    "\n",
    "    # 1. Chuẩn hoá Unicode\n",
    "    text = normalize(text)\n",
    "\n",
    "    # 2. Xử lý entity HTML phổ biến\n",
    "    html_entities = {\n",
    "        \"&quot;\": '\"',\n",
    "        \"&apos;\": \"'\",\n",
    "        \"&amp;\": \"&\",\n",
    "        \"&lt;\": \"<\",\n",
    "        \"&gt;\": \">\"\n",
    "    }\n",
    "    for k, v in html_entities.items():\n",
    "        text = text.replace(k, v)\n",
    "\n",
    "    # 3. Xoá các pattern đặc biệt như \"***\", \"---\"\n",
    "    for pattern in clean_patterns:\n",
    "        text = text.replace(pattern, \"\")\n",
    "\n",
    "    # 4. Xoá nội dung trong ngoặc kép hoặc đơn\n",
    "    text = re.sub(r'\"[^\"]*\"', '', text)\n",
    "    text = re.sub(r\"'[^']*'\", '', text)\n",
    "\n",
    "    # 5. Xoá các đường link\n",
    "    text = re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text)\n",
    "\n",
    "    # 6. Xoá dấu trang hoặc ký tự đặc biệt (ví dụ từ OCR)\n",
    "    text = re.sub(r\"[\\u200b\\u200e\\u202a\\u202c]+\", \"\", text)  # các ký tự ẩn\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # gom khoảng trắng\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def clean_page(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Làm sạch từng trang OCR: bỏ header/trailer và nối dòng.\n",
    "    \"\"\"\n",
    "    text = normalize(text)\n",
    "    # Loại bỏ tiêu đề sách lặp lại theo trang\n",
    "    text = re.sub(r\"TRANG TỬ.*NAM HOA KINH.*\", \"\", text, flags=re.I)\n",
    "    # Nối dòng không phải đoạn\n",
    "    text = re.sub(r\"(?<!\\n)\\n(?!\\n)\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def split_paragraphs(text: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Tách đoạn bằng cách phát hiện xuống dòng kép.\n",
    "    \"\"\"\n",
    "    return [normalize(p) for p in re.split(r\"\\n\\s*\\n\", text) if p.strip()]\n",
    "\n",
    "\n",
    "def split_sentences(\n",
    "    text: str,\n",
    "    delimiters=[\".\", \"!\", \"?\", \"...\"],\n",
    "    is_clean_text: bool = True\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Tách câu theo các dấu kết thúc câu tiếng Việt.\n",
    "    \"\"\"\n",
    "\n",
    "    if is_clean_text:\n",
    "        text = clean_text(text)\n",
    "\n",
    "    sentences = []\n",
    "    current_sentence = \"\"\n",
    "    for char in text:\n",
    "        current_sentence += char\n",
    "        if char in delimiters:\n",
    "            if current_sentence.strip():\n",
    "                sentences.append(normalize(current_sentence))\n",
    "            current_sentence = \"\"\n",
    "\n",
    "    if current_sentence.strip():\n",
    "        sentences.append(normalize(current_sentence))\n",
    "\n",
    "    return sentences\n",
    "\n",
    "def detect_sections(pages):\n",
    "    section_pattern = re.compile(\n",
    "        r\"^(PHẦN|CHƯƠNG)\\s+[IVXLCDM\\d]+\\.*\\s+.+$\", re.MULTILINE\n",
    "    )\n",
    "    sections = []\n",
    "    current = {\"name\": \"Giới thiệu\", \"pages\": []}\n",
    "\n",
    "    for i, txt in enumerate(pages, 1):\n",
    "        matches = section_pattern.findall(txt)\n",
    "        if matches:\n",
    "            if current[\"pages\"]:\n",
    "                sections.append(current)\n",
    "            title = re.findall(section_pattern, txt)[0]\n",
    "            current = {\"name\": normalize(title), \"pages\": [(i, txt)]}\n",
    "        else:\n",
    "            current[\"pages\"].append((i, txt))\n",
    "    sections.append(current)\n",
    "    return sections\n",
    "\n",
    "\n",
    "# ──────────────── GHI FILE XML ĐẸP ────────────────\n",
    "def write_pretty_xml(tree, out_path):\n",
    "    pretty = minidom.parseString(ET.tostring(tree.getroot(), encoding=\"utf-8\"))\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(pretty.toprettyxml(indent=\"  \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ad08ffcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_underthesea(text: str) -> list[dict]:\n",
    "    result = ner(text, deep=True)\n",
    "    return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "68a8ccb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────── HÀM CHÍNH ────────────────\n",
    "def build_xml_for_book(pdf_path, des=\"output_xml\", code: str = \"\"):\n",
    "    \"\"\"\n",
    "    Tạo file XML từ PDF, chia theo section → page → câu, giữ nguyên số trang gốc.\n",
    "    \"\"\"\n",
    "    # Bước 1: Xác định ID sách\n",
    "    book_name = Path(pdf_path).stem.upper().replace(\"-\", \"_\").replace(\" \", \"_\")\n",
    "    book_id = book_name if book_name in BOOK_METADATA else f\"{book_name}_AUTO\"\n",
    "\n",
    "    # Bước 2: Đọc file PDF\n",
    "    doc = pymupdf.open(pdf_path)\n",
    "    pages_text = [clean_page(p.get_text()) for p in doc]\n",
    "\n",
    "    # Bước 3: Tách các section\n",
    "    sections = detect_sections(pages_text)\n",
    "\n",
    "    # Bước 4: Tạo gốc XML\n",
    "    root = ET.Element(\"root\")\n",
    "    file_el = ET.SubElement(root, \"FILE\", ID=book_id)\n",
    "\n",
    "    # Metadata\n",
    "    meta_info = BOOK_METADATA.get(book_id, {})\n",
    "    meta = ET.SubElement(file_el, \"meta\")\n",
    "    ET.SubElement(meta, \"TITLE\").text = meta_info.get(\"TITLE\", book_id.title())\n",
    "    ET.SubElement(meta, \"VOLUME\").text = meta_info.get(\"VOLUME\", \"\")\n",
    "    ET.SubElement(meta, \"AUTHOR\").text = meta_info.get(\"AUTHOR\", \"Không rõ\")\n",
    "    ET.SubElement(meta, \"PERIOD\").text = meta_info.get(\"PERIOD\", \"Không rõ\")\n",
    "    ET.SubElement(meta, \"LANGUAGE\").text = meta_info.get(\"LANGUAGE\", \"vi\")\n",
    "    ET.SubElement(meta, \"SOURCE\").text = meta_info.get(\"SOURCE\", \"Tự động\")\n",
    "\n",
    "    # Bước 5: Tạo các section\n",
    "    for sect_id, section in enumerate(sections):\n",
    "        sect_el = ET.SubElement(\n",
    "            root, \"SECT\", ID=f\"{code}.{sect_id:03}\", NAME=section[\"name\"]\n",
    "        )\n",
    "\n",
    "        for page_num, page_text in section[\"pages\"]:\n",
    "            page_el = ET.SubElement(\n",
    "                sect_el, \"PAGE\", ID=f\"{code}.{sect_id:03}.{page_num:03}\"\n",
    "            )\n",
    "            # Split câu theo từng trang gốc\n",
    "            sentences = split_sentences(page_text)\n",
    "            for sent_id, sent in enumerate(sentences, 1):\n",
    "                stc_el = ET.SubElement(\n",
    "                    page_el,\n",
    "                    \"STC\",\n",
    "                    ID=f\"{code}.{sect_id:03}.{page_num:03}.{sent_id:02}\",\n",
    "                )\n",
    "                stc_el.text = sent\n",
    "\n",
    "                ner_entity = ner_underthesea(sent)\n",
    "                valid_entities = [\n",
    "                    ent for ent in ner_entity\n",
    "                    if ent.get(\"entity\", \"\").split(\"-\")[-1] in BASED_ENTITY_GROUPS\n",
    "                ]\n",
    "\n",
    "                if valid_entities:\n",
    "                    ner_el = ET.SubElement(stc_el, \"NER\")\n",
    "                    for entity in valid_entities:\n",
    "                        base_entity = entity[\"entity\"].split(\"-\")[-1]\n",
    "                        ET.SubElement(\n",
    "                            ner_el,\n",
    "                            \"ENTITY\",\n",
    "                            TYPE=base_entity,\n",
    "                            START=str(entity.get(\"start\")),\n",
    "                            END=str(entity.get(\"end\")),\n",
    "                        ).text = entity.get(\"word\")\n",
    "                        \n",
    "\n",
    "    # Bước 6: Ghi XML\n",
    "    tree = ET.ElementTree(root)\n",
    "    write_pretty_xml(tree, des)\n",
    "    print(f\"✅ Xuất file XML: {des}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "98d6c80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Xuất file XML: nam_hoa_kinh_1_sentence.xml\n"
     ]
    }
   ],
   "source": [
    "pdf_path = \"/home/octoopt/workspace/projects/learn-from-basics/the-notes/test/TrangTu_NamHoaKinh_VanAnh/Nam-hoa-kinh.pdf\"\n",
    "code = \"PKS_001\"\n",
    "mode = \"sentence\"\n",
    "des = f\"nam_hoa_kinh_1_{mode}.xml\"\n",
    "\n",
    "build_xml_for_book(pdf_path, des, code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3cf4e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sentences: 6760\n",
      "\n",
      "Number of sentences per section:\n",
      "- Giới thiệu: 6760 sentences\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "\n",
    "def count_sentences(xml_file_path):\n",
    "    # Parse the XML file\n",
    "    tree = ET.parse(xml_file_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Find all STC elements (sentences)\n",
    "    sentences = root.findall(\".//STC\")\n",
    "\n",
    "    # Count the sentences\n",
    "    total_sentences = len(sentences)\n",
    "\n",
    "    # Count sentences per section\n",
    "    sections = {}\n",
    "    for section in root.findall(\".//SECT\"):\n",
    "        section_name = section.get(\"NAME\", \"Unknown\")\n",
    "        section_sentences = section.findall(\".//STC\")\n",
    "        sections[section_name] = len(section_sentences)\n",
    "\n",
    "    return total_sentences, sections\n",
    "\n",
    "\n",
    "# Path to your XML file\n",
    "xml_file = \"/home/octoopt/workspace/projects/learn-from-basics/the-notes/others/Trang-tu-nam-hoa-kinh_PKS_001_sentence.xml\"\n",
    "\n",
    "# Get the counts\n",
    "total, section_counts = count_sentences(xml_file)\n",
    "\n",
    "print(f\"Total number of sentences: {total}\")\n",
    "print(\"\\nNumber of sentences per section:\")\n",
    "for section, count in section_counts.items():\n",
    "    print(f\"- {section}: {count} sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "318fb0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document('/home/octoopt/workspace/projects/learn-from-basics/nlp-vietnamese-phd/temp/Nam-hoa-kinh.pdf')\n",
      "{'title': None, 'author': None, 'subject': None, 'keywords': None, 'creator': None, 'producer': 'iLovePDF', 'creation_date': None, 'mod_date': \"D:20221109160826+07'00'\", 'format': 'PDF 1.4', 'page_count': 139}\n"
     ]
    }
   ],
   "source": [
    "def get_pdf_metadata(pdf_path):\n",
    "    \"\"\"\n",
    "    Extract metadata from a PDF file using PyMuPDF.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing PDF metadata\n",
    "    \"\"\"\n",
    "    doc = pymupdf.open(pdf_path)\n",
    "    print(doc)\n",
    "    meta = doc.metadata\n",
    "\n",
    "    # Convert to a dictionary and clean up None values\n",
    "    metadata = {\n",
    "        \"title\": meta.get(\"title\", \"\").strip() or None,\n",
    "        \"author\": meta.get(\"author\", \"\").strip() or None,\n",
    "        \"subject\": meta.get(\"subject\", \"\").strip() or None,\n",
    "        \"keywords\": meta.get(\"keywords\", \"\").strip() or None,\n",
    "        \"creator\": meta.get(\"creator\", \"\").strip() or None,\n",
    "        \"producer\": meta.get(\"producer\", \"\").strip() or None,\n",
    "        \"creation_date\": meta.get(\"creationDate\", \"\").strip() or None,\n",
    "        \"mod_date\": meta.get(\"modDate\", \"\").strip() or None,\n",
    "        \"format\": meta.get(\"format\", \"\").strip() or None,\n",
    "        # 'encryption': meta.get('encryption', '\\n').strip() or None,\n",
    "        \"page_count\": len(doc),\n",
    "    }\n",
    "\n",
    "    doc.close()\n",
    "    return metadata\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "pdf_path = \"/home/octoopt/workspace/projects/learn-from-basics/nlp-vietnamese-phd/temp/Nam-hoa-kinh.pdf\"\n",
    "metadata = get_pdf_metadata(pdf_path)\n",
    "print(metadata)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
