{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a94afe4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pymupdf\n",
    "import xml.etree.ElementTree as ET\n",
    "from xml.dom import minidom\n",
    "import re\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "\n",
    "from openai import OpenAI\n",
    "from underthesea import ner\n",
    "\n",
    "# ──────────────── CẤU HÌNH ────────────────\n",
    "BOOK_METADATA = {\n",
    "    \"NAM_HOA_KINH\": {\n",
    "        \"TITLE\": \"Nam Hoa Kinh\",\n",
    "        \"VOLUME\": \"\",\n",
    "        \"AUTHOR\": \"Trang Tử\",\n",
    "        \"PERIOD\": \"Chiến Quốc\",\n",
    "        \"LANGUAGE\": \"vi\",\n",
    "        \"SOURCE\": \"thuviensach.vn\",\n",
    "    },\n",
    "    \"TRANG_TU_NAM_HOA_KINH\": {\n",
    "        \"TITLE\": \"Trang Tử Nam Hoa Kinh\",\n",
    "        \"VOLUME\": \"\",\n",
    "        \"AUTHOR\": \"Nguyễn Kim Vỹ\",\n",
    "        \"PERIOD\": \"Chiến Quốc\",\n",
    "        \"LANGUAGE\": \"vi\",\n",
    "        \"SOURCE\": \"Nhà xuất bản văn hóa\",\n",
    "    },\n",
    "    \"TRANG_TU_NAM_HOA_KINH_2\": {\n",
    "        \"TITLE\": \"Trang Tử Nam Hoa Kinh\",\n",
    "        \"VOLUME\": \"\",\n",
    "        \"AUTHOR\": \"Thu Giang, Ngyễn Duy Cần\",\n",
    "        \"PERIOD\": \"Chiến Quốc\",\n",
    "        \"LANGUAGE\": \"vi\",\n",
    "        \"SOURCE\": \"Nhà xuất bản trẻ\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d5ed4246",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf\n",
    "import xml.etree.ElementTree as ET\n",
    "from xml.dom import minidom\n",
    "import re\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "import html\n",
    "\n",
    "# Import the improved HTML cleaning functions\n",
    "\n",
    "def remove_html_entities(text):\n",
    "    \"\"\"\n",
    "    Comprehensive HTML entity removal function that handles all variations of &quot;\n",
    "    and other HTML entities.\n",
    "    \"\"\"\n",
    "    # First try html.unescape for standard entities\n",
    "    try:\n",
    "        text = html.unescape(text)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Aggressive &quot; removal - handle all variations\n",
    "    text = text.replace(\"&quot;\", \"\")\n",
    "    text = text.replace(\"&quot\", \"\")  # Missing semicolon\n",
    "    text = text.replace(\"quot;\", \"\")  # Missing ampersand\n",
    "    text = text.replace(\"&QUOT;\", \"\")  # Uppercase\n",
    "    text = text.replace(\"&Quot;\", \"\")  # Mixed case\n",
    "\n",
    "    # Handle other common HTML entities\n",
    "    html_entities = {\n",
    "        \"&amp;\": \"&\",\n",
    "        \"&lt;\": \"<\",\n",
    "        \"&gt;\": \">\",\n",
    "        \"&apos;\": \"'\",\n",
    "        \"&nbsp;\": \" \",\n",
    "        \"&hellip;\": \"...\",\n",
    "        \"&mdash;\": \"—\",\n",
    "        \"&ndash;\": \"–\",\n",
    "        \"&ldquo;\": \"\",\n",
    "        \"&rdquo;\": \"\",\n",
    "        \"&lsquo;\": \"\",\n",
    "        \"&rsquo;\": \"\",\n",
    "    }\n",
    "\n",
    "    for entity, replacement in html_entities.items():\n",
    "        text = text.replace(entity, replacement)\n",
    "        # Also handle uppercase versions\n",
    "        text = text.replace(entity.upper(), replacement)\n",
    "\n",
    "    # Use regex to catch any remaining HTML entity patterns\n",
    "    text = re.sub(r\"&[a-zA-Z]+;\", \"\", text)\n",
    "    text = re.sub(r\"&[a-zA-Z]+\", \"\", text)  # Missing semicolon\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "KNOWN_SECTIONS = [\n",
    "    \"LỜI NÓI ĐẦU\", \"TIỂU DẪN\", \"NỘI THIÊN\", \"NGOẠI THIÊN\", \"TẠP THIÊN\",\n",
    "    \"TIÊU DIÊU DU\", \"TỀ VẬT LUẬN\", \"DƯỠNG SINH CHỦ\", \"NHÂN GIAN THẾ\",\n",
    "    \"ĐỨC SUNG PHÙ\", \"ĐẠI TÔNG SƯ\", \"ỨNG ĐẾ VƯƠNG\"\n",
    "]\n",
    "\n",
    "# ──────────────── UTILITY FUNCTIONS ────────────────\n",
    "def normalize(s: str) -> str:\n",
    "    return unicodedata.normalize(\"NFC\", s.strip())\n",
    "\n",
    "def is_chinese(text: str) -> bool:\n",
    "    \"\"\"Check if text contains Chinese characters.\"\"\"\n",
    "    chinese_chars = sum(1 for c in text if 0x4E00 <= ord(c) <= 0x9FFF)\n",
    "    total_chars = len([c for c in text if c.isalnum()])\n",
    "    return chinese_chars > 0 and (chinese_chars / max(total_chars, 1)) > 0.3\n",
    "\n",
    "def is_vietnamese(text: str) -> bool:\n",
    "    \"\"\"Check if text contains Vietnamese characters.\"\"\"\n",
    "    vietnamese_pattern = re.compile(r'[àáảãạăằắẳẵặâầấẩẫậèéẻẽẹêềếểễệìíỉĩịòóỏõọôồốổỗộơờớởỡợùúủũụưừứửữựỳýỷỹỵđĐ]')\n",
    "    return bool(vietnamese_pattern.search(text))\n",
    "\n",
    "def classify_text(text: str) -> str:\n",
    "    \"\"\"Classify text as Chinese, Vietnamese, or Mixed.\"\"\"\n",
    "    text = normalize(text)\n",
    "    has_chinese = is_chinese(text)\n",
    "    has_vietnamese = is_vietnamese(text)\n",
    "    \n",
    "    if has_chinese and has_vietnamese:\n",
    "        return \"Mixed\"\n",
    "    elif has_chinese:\n",
    "        return \"Chinese\"\n",
    "    elif has_vietnamese:\n",
    "        return \"Vietnamese\"\n",
    "    return \"Other\"\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Clean text with comprehensive HTML entity removal.\"\"\"\n",
    "    # Remove HTML entities first\n",
    "    text = remove_html_entities(text)\n",
    "    text = normalize(text)\n",
    "    \n",
    "    # Remove common OCR artifacts\n",
    "    clean_patterns = [\"999\", \"F.F.F\", \"***\", \"---\", \"___\"]\n",
    "    for pattern in clean_patterns:\n",
    "        text = text.replace(pattern, \"\")\n",
    "    \n",
    "    # Remove invisible characters\n",
    "    text = re.sub(r\"[\\u200b\\u200e\\u202a\\u202c\\ufeff]+\", \"\", text)\n",
    "    \n",
    "    # Clean up multiple spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    \n",
    "    # Remove the specific characters requested by the user\n",
    "    text = text.replace('⸈', '')\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def clean_page(text: str, known_sections: list[str]=KNOWN_SECTIONS) -> str:\n",
    "    \"\"\"Clean page OCR text.\"\"\"\n",
    "    text = normalize(text)\n",
    "    text = remove_html_entities(text)\n",
    "    \n",
    "    for section_name in known_sections:  # Detect and preserve titles\n",
    "        if section_name in text:\n",
    "            preserved_title = section_name\n",
    "            text = re.sub(rf'(?i)^{re.escape(section_name)}', preserved_title, text)  # Keep title\n",
    "            break  # Only handle the first match\n",
    "    \n",
    "    text = re.sub(r\"TRANG TỬ.*NAM HOA KINH.*\", \"\", text, flags=re.I)  # Existing removal\n",
    "    text = re.sub(r\"Trang \\d+\", \"\", text, flags=re.I)\n",
    "    text = re.sub(r\"^\\d+\\s*$\", \"\", text, flags=re.MULTILINE)\n",
    "    text = re.sub(r\"(?<!\\n)\\n(?!\\n)\", \" \", text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def split_into_sentences(text: str) -> list[str]:\n",
    "    \"\"\"Split text into sentences with better Chinese-Vietnamese handling.\"\"\"\n",
    "    text = clean_text(text)\n",
    "    \n",
    "    # Split by paragraph first\n",
    "    paragraphs = re.split(r\"\\n\\s*\\n\", text)\n",
    "    sentences = []\n",
    "    \n",
    "    for para in paragraphs:\n",
    "        if not para.strip():\n",
    "            continue\n",
    "            \n",
    "        # For each paragraph, split by common sentence delimiters\n",
    "        current_sentence = \"\"\n",
    "        i = 0\n",
    "        \n",
    "        while i < len(para):\n",
    "            char = para[i]\n",
    "            current_sentence += char\n",
    "            \n",
    "            # Check for sentence endings\n",
    "            if char in [\".\", \"!\", \"?\", \"。\", \"！\", \"？\"]:\n",
    "                # Look ahead to see if this is really end of sentence\n",
    "                next_char = para[i + 1] if i + 1 < len(para) else \"\"\n",
    "                \n",
    "                # Don't split on numbers like \"1.2\" or \"3.14\"\n",
    "                if char == \".\" and next_char.isdigit():\n",
    "                    i += 1\n",
    "                    continue\n",
    "                \n",
    "                # Don't split Chinese text on period unless followed by space/newline\n",
    "                if char == \".\" and is_chinese(current_sentence) and next_char not in [\" \", \"\\n\", \"\", \"\\t\"]:\n",
    "                    i += 1\n",
    "                    continue\n",
    "                \n",
    "                # Add sentence if not empty\n",
    "                if current_sentence.strip():\n",
    "                    cleaned = clean_text(current_sentence)\n",
    "                    if cleaned and len(cleaned) > 5:  # Filter out very short sentences\n",
    "                        sentences.append(cleaned)\n",
    "                current_sentence = \"\"\n",
    "            \n",
    "            i += 1\n",
    "        \n",
    "        # Add any remaining text\n",
    "        if current_sentence.strip():\n",
    "            cleaned = clean_text(current_sentence)\n",
    "            if cleaned and len(cleaned) > 5:\n",
    "                sentences.append(cleaned)\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "def detect_sections(pages, known_sections=KNOWN_SECTIONS):\n",
    "    \"\"\"Detect sections from table of contents.\"\"\"\n",
    "    \n",
    "    sections = []\n",
    "    current = {\"name\": \"TIÊU DIÊU DU\", \"pages\": []}  # Default section\n",
    "    \n",
    "    for i, txt in enumerate(pages, 1):\n",
    "        cleaned_txt = clean_page(txt, known_sections)\n",
    "        \n",
    "        # Look for section titles\n",
    "        found_section = False\n",
    "        for section_name in known_sections:\n",
    "            if section_name in cleaned_txt:\n",
    "                if current[\"pages\"]:\n",
    "                    sections.append(current)\n",
    "                current = {\"name\": section_name, \"pages\": [(i, cleaned_txt)]}\n",
    "                found_section = True\n",
    "                print(f\"Found section '{section_name}' on page {i}\")\n",
    "                break\n",
    "        \n",
    "        if not found_section:\n",
    "            current[\"pages\"].append((i, cleaned_txt))\n",
    "    \n",
    "    if current[\"pages\"]:\n",
    "        sections.append(current)\n",
    "    \n",
    "    print(f\"Total sections found: {len(sections)}\")\n",
    "    for section in sections:\n",
    "        print(f\"- {section['name']}: {len(section['pages'])} pages\")\n",
    "    \n",
    "    return sections\n",
    "\n",
    "def pair_chinese_vietnamese_sentences(sentences):\n",
    "    pairs = []\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(sentences):\n",
    "        current_sent = sentences[i]\n",
    "        current_lang = classify_text(current_sent)\n",
    "        \n",
    "        if i + 1 < len(sentences):\n",
    "            next_sent = sentences[i + 1]\n",
    "            next_lang = classify_text(next_sent)\n",
    "            \n",
    "            if current_lang == 'Chinese' and next_lang == 'Vietnamese':\n",
    "                pairs.append({'chinese': current_sent, 'vietnamese': next_sent})\n",
    "                i += 2  # Skip to next\n",
    "            elif current_lang == 'Vietnamese':\n",
    "                pairs.append({'chinese': None, 'vietnamese': current_sent})  # Unpaired Vietnamese\n",
    "                i += 1\n",
    "            else:\n",
    "                i += 1  # Skip non-pairs\n",
    "        else:\n",
    "            if current_lang == 'Vietnamese':  # Last sentence is unpaired Vietnamese\n",
    "                pairs.append({'chinese': None, 'vietnamese': current_sent})\n",
    "            i += 1  # Move on\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "def write_pretty_xml(tree, out_path):\n",
    "    \"\"\"Write XML with pretty formatting.\"\"\"\n",
    "    pretty = minidom.parseString(ET.tostring(tree.getroot(), encoding=\"utf-8\"))\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(pretty.toprettyxml(indent=\"  \"))\n",
    "\n",
    "def build_xml_for_book(pdf_path, metadata: dict, output_path=\"nam_hoa_kinh_parsed.xml\", code=\"PKS_001\"):\n",
    "    \"\"\"\n",
    "    Parse Nam Hoa Kinh PDF and create XML with 1:1 Chinese-Vietnamese sentence pairs.\n",
    "    \"\"\"\n",
    "    print(f\"🔄 Processing PDF: {pdf_path}\")\n",
    "    \n",
    "    # Step 1: Read PDF\n",
    "    doc = pymupdf.open(pdf_path)\n",
    "    pages_text = [clean_page(p.get_text()) for p in doc]\n",
    "    print(f\"📄 Extracted {len(pages_text)} pages\")\n",
    "    \n",
    "    # Step 2: Detect sections\n",
    "    sections = detect_sections(pages_text, known_sections=KNOWN_SECTIONS)\n",
    "    \n",
    "    # Step 3: Create XML structure\n",
    "    root = ET.Element(\"root\")\n",
    "    file_el = ET.SubElement(root, \"FILE\", ID=code)\n",
    "    \n",
    "    # Metadata\n",
    "    meta = ET.SubElement(file_el, \"meta\")\n",
    "    ET.SubElement(meta, \"TITLE\").text = metadata.get(\"TITLE\", \"\")\n",
    "    ET.SubElement(meta, \"VOLUME\").text = metadata.get(\"VOLUME\", \"\")\n",
    "    ET.SubElement(meta, \"AUTHOR\").text = metadata.get(\"AUTHOR\", \"\")\n",
    "    ET.SubElement(meta, \"PERIOD\").text = metadata.get(\"PERIOD\", \"\")\n",
    "    ET.SubElement(meta, \"LANGUAGE\").text = metadata.get(\"LANGUAGE\", \"\")\n",
    "    ET.SubElement(meta, \"TRANSLATOR\").text = metadata.get(\"TRANSLATOR\", \"\")\n",
    "    ET.SubElement(meta, \"SOURCE\").text = metadata.get(\"SOURCE\", \"\")\n",
    "    \n",
    "    # Step 4: Process sections\n",
    "    total_pairs = 0\n",
    "    \n",
    "    for sect_id, section in enumerate(sections, 1):\n",
    "        sect_el = ET.SubElement(\n",
    "            file_el, \"SECT\", ID=f\"{code}.{sect_id:03}\", NAME=section[\"name\"]\n",
    "        )\n",
    "        \n",
    "        for page_num, page_text in section[\"pages\"]:\n",
    "            if not page_text:\n",
    "                continue\n",
    "            \n",
    "            page_el = ET.SubElement(\n",
    "                sect_el, \"PAGE\", ID=f\"{code}.{sect_id:03}.{page_num:03}\"\n",
    "            )\n",
    "            \n",
    "            # Extract sentences from page\n",
    "            sentences = split_into_sentences(page_text)\n",
    "            \n",
    "            # Pair Chinese and Vietnamese sentences\n",
    "            pairs = pair_chinese_vietnamese_sentences(sentences)\n",
    "            \n",
    "            # Create STC elements\n",
    "            for sent_id, pair in enumerate(pairs, 1):\n",
    "                stc_el = ET.SubElement(\n",
    "                    page_el, \"STC\", ID=f\"{code}.{sect_id:03}.{page_num:03}.{sent_id:02}\"\n",
    "                )\n",
    "                \n",
    "                if pair[\"chinese\"] and pair[\"vietnamese\"]:\n",
    "                    # Both Chinese and Vietnamese - use C and V tags\n",
    "                    ET.SubElement(stc_el, \"C\").text = pair[\"chinese\"]\n",
    "                    ET.SubElement(stc_el, \"V\").text = pair[\"vietnamese\"]\n",
    "                elif pair[\"chinese\"]:\n",
    "                    # Only Chinese - use C tag\n",
    "                    ET.SubElement(stc_el, \"C\").text = pair[\"chinese\"]\n",
    "                elif pair[\"vietnamese\"]:\n",
    "                    stc_el.text = pair[\"vietnamese\"]  # Set text directly in STC as per user request\n",
    "                \n",
    "                total_pairs += 1\n",
    "    \n",
    "    # Step 5: Write XML\n",
    "    tree = ET.ElementTree(root)\n",
    "    write_pretty_xml(tree, output_path)\n",
    "    \n",
    "    print(f\"✅ Created XML file: {output_path}\")\n",
    "    print(f\"📊 Total sentence pairs: {total_pairs}\")\n",
    "    \n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "98d6c80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Processing PDF: /home/octoopt/workspace/projects/learn-from-basics/nlp-vietnamese-phd/temp/Trang-tu-nam-hoa-kinh.pdf\n",
      "📄 Extracted 295 pages\n",
      "Found section 'LỜI NÓI ĐẦU' on page 2\n",
      "Found section 'LỜI NÓI ĐẦU' on page 8\n",
      "Found section 'TIỂU DẪN' on page 11\n",
      "Found section 'NỘI THIÊN' on page 57\n",
      "Found section 'TIÊU DIÊU DU' on page 58\n",
      "Found section 'TIÊU DIÊU DU' on page 60\n",
      "Found section 'TỀ VẬT LUẬN' on page 81\n",
      "Found section 'TỀ VẬT LUẬN' on page 85\n",
      "Found section 'DƯỠNG SINH CHỦ' on page 127\n",
      "Found section 'DƯỠNG SINH CHỦ' on page 128\n",
      "Found section 'ĐỨC SUNG PHÙ' on page 138\n",
      "Found section 'ĐỨC SUNG PHÙ' on page 140\n",
      "Found section 'ĐẠI TÔNG SƯ' on page 157\n",
      "Found section 'ỨNG ĐẾ VƯƠNG' on page 183\n",
      "Found section 'NGOẠI THIÊN' on page 199\n",
      "Found section 'LỜI NÓI ĐẦU' on page 200\n",
      "Found section 'LỜI NÓI ĐẦU' on page 292\n",
      "Total sections found: 18\n",
      "- TIÊU DIÊU DU: 1 pages\n",
      "- LỜI NÓI ĐẦU: 6 pages\n",
      "- LỜI NÓI ĐẦU: 3 pages\n",
      "- TIỂU DẪN: 46 pages\n",
      "- NỘI THIÊN: 1 pages\n",
      "- TIÊU DIÊU DU: 2 pages\n",
      "- TIÊU DIÊU DU: 21 pages\n",
      "- TỀ VẬT LUẬN: 4 pages\n",
      "- TỀ VẬT LUẬN: 42 pages\n",
      "- DƯỠNG SINH CHỦ: 1 pages\n",
      "- DƯỠNG SINH CHỦ: 10 pages\n",
      "- ĐỨC SUNG PHÙ: 2 pages\n",
      "- ĐỨC SUNG PHÙ: 17 pages\n",
      "- ĐẠI TÔNG SƯ: 26 pages\n",
      "- ỨNG ĐẾ VƯƠNG: 16 pages\n",
      "- NGOẠI THIÊN: 1 pages\n",
      "- LỜI NÓI ĐẦU: 92 pages\n",
      "- LỜI NÓI ĐẦU: 4 pages\n",
      "✅ Created XML file: PAS_003_Trang-tu-nam-hoa-kinh.xml\n",
      "📊 Total sentence pairs: 5553\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'PAS_003_Trang-tu-nam-hoa-kinh.xml'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_path = \"/home/octoopt/workspace/projects/learn-from-basics/nlp-vietnamese-phd/temp/Trang-tu-nam-hoa-kinh.pdf\"\n",
    "code = \"PAS_003\"\n",
    "des = f\"{code}_Trang-tu-nam-hoa-kinh.xml\"\n",
    "\n",
    "metadata = BOOK_METADATA[\"TRANG_TU_NAM_HOA_KINH_2\"]\n",
    "\n",
    "build_xml_for_book(pdf_path, metadata, des, code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3cf4e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sentences: 5548\n",
      "\n",
      "Number of sentences per section:\n",
      "- TIÊU DIÊU DU: 437 sentences\n",
      "- LỜI NÓI ĐẦU: 2 sentences\n",
      "- TIỂU DẪN: 893 sentences\n",
      "- NỘI THIÊN: 0 sentences\n",
      "- TỀ VẬT LUẬN: 857 sentences\n",
      "- DƯỠNG SINH CHỦ: 232 sentences\n",
      "- ĐỨC SUNG PHÙ: 330 sentences\n",
      "- ĐẠI TÔNG SƯ: 639 sentences\n",
      "- ỨNG ĐẾ VƯƠNG: 323 sentences\n",
      "- NGOẠI THIÊN: 0 sentences\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "\n",
    "def count_sentences(xml_file_path):\n",
    "    # Parse the XML file\n",
    "    tree = ET.parse(xml_file_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Find all STC elements (sentences)\n",
    "    sentences = root.findall(\".//STC\")\n",
    "\n",
    "    # Count the sentences\n",
    "    total_sentences = len(sentences)\n",
    "\n",
    "    # Count sentences per section\n",
    "    sections = {}\n",
    "    for section in root.findall(\".//SECT\"):\n",
    "        section_name = section.get(\"NAME\", \"Unknown\")\n",
    "        section_sentences = section.findall(\".//STC\")\n",
    "        sections[section_name] = len(section_sentences)\n",
    "\n",
    "    return total_sentences, sections\n",
    "\n",
    "\n",
    "# Path to your XML file\n",
    "xml_file = \"/home/octoopt/workspace/projects/learn-from-basics/nlp-vietnamese-phd/PAS_003_Trang-tu-nam-hoa-kinh.xml\"\n",
    "\n",
    "# Get the counts\n",
    "total, section_counts = count_sentences(xml_file)\n",
    "\n",
    "print(f\"Total number of sentences: {total}\")\n",
    "print(\"\\nNumber of sentences per section:\")\n",
    "for section, count in section_counts.items():\n",
    "    print(f\"- {section}: {count} sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "318fb0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document('/home/octoopt/workspace/projects/learn-from-basics/nlp-vietnamese-phd/temp/Nam-hoa-kinh.pdf')\n",
      "{'title': None, 'author': None, 'subject': None, 'keywords': None, 'creator': None, 'producer': 'iLovePDF', 'creation_date': None, 'mod_date': \"D:20221109160826+07'00'\", 'format': 'PDF 1.4', 'page_count': 139}\n"
     ]
    }
   ],
   "source": [
    "def get_pdf_metadata(pdf_path):\n",
    "    \"\"\"\n",
    "    Extract metadata from a PDF file using PyMuPDF.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing PDF metadata\n",
    "    \"\"\"\n",
    "    doc = pymupdf.open(pdf_path)\n",
    "    print(doc)\n",
    "    meta = doc.metadata\n",
    "\n",
    "    # Convert to a dictionary and clean up None values\n",
    "    metadata = {\n",
    "        \"title\": meta.get(\"title\", \"\").strip() or None,\n",
    "        \"author\": meta.get(\"author\", \"\").strip() or None,\n",
    "        \"subject\": meta.get(\"subject\", \"\").strip() or None,\n",
    "        \"keywords\": meta.get(\"keywords\", \"\").strip() or None,\n",
    "        \"creator\": meta.get(\"creator\", \"\").strip() or None,\n",
    "        \"producer\": meta.get(\"producer\", \"\").strip() or None,\n",
    "        \"creation_date\": meta.get(\"creationDate\", \"\").strip() or None,\n",
    "        \"mod_date\": meta.get(\"modDate\", \"\").strip() or None,\n",
    "        \"format\": meta.get(\"format\", \"\").strip() or None,\n",
    "        # 'encryption': meta.get('encryption', '\\n').strip() or None,\n",
    "        \"page_count\": len(doc),\n",
    "    }\n",
    "\n",
    "    doc.close()\n",
    "    return metadata\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "pdf_path = \"/home/octoopt/workspace/projects/learn-from-basics/nlp-vietnamese-phd/temp/Nam-hoa-kinh.pdf\"\n",
    "metadata = get_pdf_metadata(pdf_path)\n",
    "print(metadata)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
