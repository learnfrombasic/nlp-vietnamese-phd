{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a94afe4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pymupdf\n",
    "import xml.etree.ElementTree as ET\n",
    "from xml.dom import minidom\n",
    "import re\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "\n",
    "from openai import OpenAI\n",
    "from underthesea import ner\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Cáº¤U HÃŒNH â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "BOOK_METADATA = {\n",
    "    \"NAM_HOA_KINH\": {\n",
    "        \"TITLE\": \"Nam Hoa Kinh\",\n",
    "        \"VOLUME\": \"\",\n",
    "        \"AUTHOR\": \"Trang Tá»­\",\n",
    "        \"PERIOD\": \"Chiáº¿n Quá»‘c\",\n",
    "        \"LANGUAGE\": \"vi\",\n",
    "        \"SOURCE\": \"thuviensach.vn\",\n",
    "    },\n",
    "    \"TRANG_TU_NAM_HOA_KINH\": {\n",
    "        \"TITLE\": \"Trang Tá»­ Nam Hoa Kinh\",\n",
    "        \"VOLUME\": \"\",\n",
    "        \"AUTHOR\": \"Nguyá»…n Kim Vá»¹\",\n",
    "        \"PERIOD\": \"Chiáº¿n Quá»‘c\",\n",
    "        \"LANGUAGE\": \"vi\",\n",
    "        \"SOURCE\": \"NhÃ  xuáº¥t báº£n vÄƒn hÃ³a\",\n",
    "    },\n",
    "    \"TRANG_TU_NAM_HOA_KINH_2\": {\n",
    "        \"TITLE\": \"Trang Tá»­ Nam Hoa Kinh\",\n",
    "        \"VOLUME\": \"\",\n",
    "        \"AUTHOR\": \"Thu Giang, Ngyá»…n Duy Cáº§n\",\n",
    "        \"PERIOD\": \"Chiáº¿n Quá»‘c\",\n",
    "        \"LANGUAGE\": \"vi\",\n",
    "        \"SOURCE\": \"NhÃ  xuáº¥t báº£n tráº»\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d5ed4246",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf\n",
    "import xml.etree.ElementTree as ET\n",
    "from xml.dom import minidom\n",
    "import re\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "import html\n",
    "\n",
    "# Import the improved HTML cleaning functions\n",
    "\n",
    "def remove_html_entities(text):\n",
    "    \"\"\"\n",
    "    Comprehensive HTML entity removal function that handles all variations of &quot;\n",
    "    and other HTML entities.\n",
    "    \"\"\"\n",
    "    # First try html.unescape for standard entities\n",
    "    try:\n",
    "        text = html.unescape(text)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Aggressive &quot; removal - handle all variations\n",
    "    text = text.replace(\"&quot;\", \"\")\n",
    "    text = text.replace(\"&quot\", \"\")  # Missing semicolon\n",
    "    text = text.replace(\"quot;\", \"\")  # Missing ampersand\n",
    "    text = text.replace(\"&QUOT;\", \"\")  # Uppercase\n",
    "    text = text.replace(\"&Quot;\", \"\")  # Mixed case\n",
    "\n",
    "    # Handle other common HTML entities\n",
    "    html_entities = {\n",
    "        \"&amp;\": \"&\",\n",
    "        \"&lt;\": \"<\",\n",
    "        \"&gt;\": \">\",\n",
    "        \"&apos;\": \"'\",\n",
    "        \"&nbsp;\": \" \",\n",
    "        \"&hellip;\": \"...\",\n",
    "        \"&mdash;\": \"â€”\",\n",
    "        \"&ndash;\": \"â€“\",\n",
    "        \"&ldquo;\": \"\",\n",
    "        \"&rdquo;\": \"\",\n",
    "        \"&lsquo;\": \"\",\n",
    "        \"&rsquo;\": \"\",\n",
    "    }\n",
    "\n",
    "    for entity, replacement in html_entities.items():\n",
    "        text = text.replace(entity, replacement)\n",
    "        # Also handle uppercase versions\n",
    "        text = text.replace(entity.upper(), replacement)\n",
    "\n",
    "    # Use regex to catch any remaining HTML entity patterns\n",
    "    text = re.sub(r\"&[a-zA-Z]+;\", \"\", text)\n",
    "    text = re.sub(r\"&[a-zA-Z]+\", \"\", text)  # Missing semicolon\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "KNOWN_SECTIONS = [\n",
    "    \"Lá»œI NÃ“I Äáº¦U\", \"TIá»‚U DáºªN\", \"Ná»˜I THIÃŠN\", \"NGOáº I THIÃŠN\", \"Táº P THIÃŠN\",\n",
    "    \"TIÃŠU DIÃŠU DU\", \"Tá»€ Váº¬T LUáº¬N\", \"DÆ¯á» NG SINH CHá»¦\", \"NHÃ‚N GIAN THáº¾\",\n",
    "    \"Äá»¨C SUNG PHÃ™\", \"Äáº I TÃ”NG SÆ¯\", \"á»¨NG Äáº¾ VÆ¯Æ NG\"\n",
    "]\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ UTILITY FUNCTIONS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def normalize(s: str) -> str:\n",
    "    return unicodedata.normalize(\"NFC\", s.strip())\n",
    "\n",
    "def is_chinese(text: str) -> bool:\n",
    "    \"\"\"Check if text contains Chinese characters.\"\"\"\n",
    "    chinese_chars = sum(1 for c in text if 0x4E00 <= ord(c) <= 0x9FFF)\n",
    "    total_chars = len([c for c in text if c.isalnum()])\n",
    "    return chinese_chars > 0 and (chinese_chars / max(total_chars, 1)) > 0.3\n",
    "\n",
    "def is_vietnamese(text: str) -> bool:\n",
    "    \"\"\"Check if text contains Vietnamese characters.\"\"\"\n",
    "    vietnamese_pattern = re.compile(r'[Ã Ã¡áº£Ã£áº¡Äƒáº±áº¯áº³áºµáº·Ã¢áº§áº¥áº©áº«áº­Ã¨Ã©áº»áº½áº¹Ãªá»áº¿á»ƒá»…á»‡Ã¬Ã­á»‰Ä©á»‹Ã²Ã³á»Ãµá»Ã´á»“á»‘á»•á»—á»™Æ¡á»á»›á»Ÿá»¡á»£Ã¹Ãºá»§Å©á»¥Æ°á»«á»©á»­á»¯á»±á»³Ã½á»·á»¹á»µÄ‘Ä]')\n",
    "    return bool(vietnamese_pattern.search(text))\n",
    "\n",
    "def classify_text(text: str) -> str:\n",
    "    \"\"\"Classify text as Chinese, Vietnamese, or Mixed.\"\"\"\n",
    "    text = normalize(text)\n",
    "    has_chinese = is_chinese(text)\n",
    "    has_vietnamese = is_vietnamese(text)\n",
    "    \n",
    "    if has_chinese and has_vietnamese:\n",
    "        return \"Mixed\"\n",
    "    elif has_chinese:\n",
    "        return \"Chinese\"\n",
    "    elif has_vietnamese:\n",
    "        return \"Vietnamese\"\n",
    "    return \"Other\"\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Clean text with comprehensive HTML entity removal.\"\"\"\n",
    "    # Remove HTML entities first\n",
    "    text = remove_html_entities(text)\n",
    "    text = normalize(text)\n",
    "    \n",
    "    # Remove common OCR artifacts\n",
    "    clean_patterns = [\"999\", \"F.F.F\", \"***\", \"---\", \"___\"]\n",
    "    for pattern in clean_patterns:\n",
    "        text = text.replace(pattern, \"\")\n",
    "    \n",
    "    # Remove invisible characters\n",
    "    text = re.sub(r\"[\\u200b\\u200e\\u202a\\u202c\\ufeff]+\", \"\", text)\n",
    "    \n",
    "    # Clean up multiple spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    \n",
    "    # Remove the specific characters requested by the user\n",
    "    text = text.replace('â¸ˆ', '')\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def clean_page(text: str, known_sections: list[str]=KNOWN_SECTIONS) -> str:\n",
    "    \"\"\"Clean page OCR text.\"\"\"\n",
    "    text = normalize(text)\n",
    "    text = remove_html_entities(text)\n",
    "    \n",
    "    for section_name in known_sections:  # Detect and preserve titles\n",
    "        if section_name in text:\n",
    "            preserved_title = section_name\n",
    "            text = re.sub(rf'(?i)^{re.escape(section_name)}', preserved_title, text)  # Keep title\n",
    "            break  # Only handle the first match\n",
    "    \n",
    "    text = re.sub(r\"TRANG Tá»¬.*NAM HOA KINH.*\", \"\", text, flags=re.I)  # Existing removal\n",
    "    text = re.sub(r\"Trang \\d+\", \"\", text, flags=re.I)\n",
    "    text = re.sub(r\"^\\d+\\s*$\", \"\", text, flags=re.MULTILINE)\n",
    "    text = re.sub(r\"(?<!\\n)\\n(?!\\n)\", \" \", text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def split_into_sentences(text: str) -> list[str]:\n",
    "    \"\"\"Split text into sentences with better Chinese-Vietnamese handling.\"\"\"\n",
    "    text = clean_text(text)\n",
    "    \n",
    "    # Split by paragraph first\n",
    "    paragraphs = re.split(r\"\\n\\s*\\n\", text)\n",
    "    sentences = []\n",
    "    \n",
    "    for para in paragraphs:\n",
    "        if not para.strip():\n",
    "            continue\n",
    "            \n",
    "        # For each paragraph, split by common sentence delimiters\n",
    "        current_sentence = \"\"\n",
    "        i = 0\n",
    "        \n",
    "        while i < len(para):\n",
    "            char = para[i]\n",
    "            current_sentence += char\n",
    "            \n",
    "            # Check for sentence endings\n",
    "            if char in [\".\", \"!\", \"?\", \"ã€‚\", \"ï¼\", \"ï¼Ÿ\"]:\n",
    "                # Look ahead to see if this is really end of sentence\n",
    "                next_char = para[i + 1] if i + 1 < len(para) else \"\"\n",
    "                \n",
    "                # Don't split on numbers like \"1.2\" or \"3.14\"\n",
    "                if char == \".\" and next_char.isdigit():\n",
    "                    i += 1\n",
    "                    continue\n",
    "                \n",
    "                # Don't split Chinese text on period unless followed by space/newline\n",
    "                if char == \".\" and is_chinese(current_sentence) and next_char not in [\" \", \"\\n\", \"\", \"\\t\"]:\n",
    "                    i += 1\n",
    "                    continue\n",
    "                \n",
    "                # Add sentence if not empty\n",
    "                if current_sentence.strip():\n",
    "                    cleaned = clean_text(current_sentence)\n",
    "                    if cleaned and len(cleaned) > 5:  # Filter out very short sentences\n",
    "                        sentences.append(cleaned)\n",
    "                current_sentence = \"\"\n",
    "            \n",
    "            i += 1\n",
    "        \n",
    "        # Add any remaining text\n",
    "        if current_sentence.strip():\n",
    "            cleaned = clean_text(current_sentence)\n",
    "            if cleaned and len(cleaned) > 5:\n",
    "                sentences.append(cleaned)\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "def detect_sections(pages, known_sections=KNOWN_SECTIONS):\n",
    "    \"\"\"Detect sections from table of contents.\"\"\"\n",
    "    \n",
    "    sections = []\n",
    "    current = {\"name\": \"TIÃŠU DIÃŠU DU\", \"pages\": []}  # Default section\n",
    "    \n",
    "    for i, txt in enumerate(pages, 1):\n",
    "        cleaned_txt = clean_page(txt, known_sections)\n",
    "        \n",
    "        # Look for section titles\n",
    "        found_section = False\n",
    "        for section_name in known_sections:\n",
    "            if section_name in cleaned_txt:\n",
    "                if current[\"pages\"]:\n",
    "                    sections.append(current)\n",
    "                current = {\"name\": section_name, \"pages\": [(i, cleaned_txt)]}\n",
    "                found_section = True\n",
    "                print(f\"Found section '{section_name}' on page {i}\")\n",
    "                break\n",
    "        \n",
    "        if not found_section:\n",
    "            current[\"pages\"].append((i, cleaned_txt))\n",
    "    \n",
    "    if current[\"pages\"]:\n",
    "        sections.append(current)\n",
    "    \n",
    "    print(f\"Total sections found: {len(sections)}\")\n",
    "    for section in sections:\n",
    "        print(f\"- {section['name']}: {len(section['pages'])} pages\")\n",
    "    \n",
    "    return sections\n",
    "\n",
    "def pair_chinese_vietnamese_sentences(sentences):\n",
    "    pairs = []\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(sentences):\n",
    "        current_sent = sentences[i]\n",
    "        current_lang = classify_text(current_sent)\n",
    "        \n",
    "        if i + 1 < len(sentences):\n",
    "            next_sent = sentences[i + 1]\n",
    "            next_lang = classify_text(next_sent)\n",
    "            \n",
    "            if current_lang == 'Chinese' and next_lang == 'Vietnamese':\n",
    "                pairs.append({'chinese': current_sent, 'vietnamese': next_sent})\n",
    "                i += 2  # Skip to next\n",
    "            elif current_lang == 'Vietnamese':\n",
    "                pairs.append({'chinese': None, 'vietnamese': current_sent})  # Unpaired Vietnamese\n",
    "                i += 1\n",
    "            else:\n",
    "                i += 1  # Skip non-pairs\n",
    "        else:\n",
    "            if current_lang == 'Vietnamese':  # Last sentence is unpaired Vietnamese\n",
    "                pairs.append({'chinese': None, 'vietnamese': current_sent})\n",
    "            i += 1  # Move on\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "def write_pretty_xml(tree, out_path):\n",
    "    \"\"\"Write XML with pretty formatting.\"\"\"\n",
    "    pretty = minidom.parseString(ET.tostring(tree.getroot(), encoding=\"utf-8\"))\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(pretty.toprettyxml(indent=\"  \"))\n",
    "\n",
    "def build_xml_for_book(pdf_path, metadata: dict, output_path=\"nam_hoa_kinh_parsed.xml\", code=\"PKS_001\"):\n",
    "    \"\"\"\n",
    "    Parse Nam Hoa Kinh PDF and create XML with 1:1 Chinese-Vietnamese sentence pairs.\n",
    "    \"\"\"\n",
    "    print(f\"ðŸ”„ Processing PDF: {pdf_path}\")\n",
    "    \n",
    "    # Step 1: Read PDF\n",
    "    doc = pymupdf.open(pdf_path)\n",
    "    pages_text = [clean_page(p.get_text()) for p in doc]\n",
    "    print(f\"ðŸ“„ Extracted {len(pages_text)} pages\")\n",
    "    \n",
    "    # Step 2: Detect sections\n",
    "    sections = detect_sections(pages_text, known_sections=KNOWN_SECTIONS)\n",
    "    \n",
    "    # Step 3: Create XML structure\n",
    "    root = ET.Element(\"root\")\n",
    "    file_el = ET.SubElement(root, \"FILE\", ID=code)\n",
    "    \n",
    "    # Metadata\n",
    "    meta = ET.SubElement(file_el, \"meta\")\n",
    "    ET.SubElement(meta, \"TITLE\").text = metadata.get(\"TITLE\", \"\")\n",
    "    ET.SubElement(meta, \"VOLUME\").text = metadata.get(\"VOLUME\", \"\")\n",
    "    ET.SubElement(meta, \"AUTHOR\").text = metadata.get(\"AUTHOR\", \"\")\n",
    "    ET.SubElement(meta, \"PERIOD\").text = metadata.get(\"PERIOD\", \"\")\n",
    "    ET.SubElement(meta, \"LANGUAGE\").text = metadata.get(\"LANGUAGE\", \"\")\n",
    "    ET.SubElement(meta, \"TRANSLATOR\").text = metadata.get(\"TRANSLATOR\", \"\")\n",
    "    ET.SubElement(meta, \"SOURCE\").text = metadata.get(\"SOURCE\", \"\")\n",
    "    \n",
    "    # Step 4: Process sections\n",
    "    total_pairs = 0\n",
    "    \n",
    "    for sect_id, section in enumerate(sections, 1):\n",
    "        sect_el = ET.SubElement(\n",
    "            file_el, \"SECT\", ID=f\"{code}.{sect_id:03}\", NAME=section[\"name\"]\n",
    "        )\n",
    "        \n",
    "        for page_num, page_text in section[\"pages\"]:\n",
    "            if not page_text:\n",
    "                continue\n",
    "            \n",
    "            page_el = ET.SubElement(\n",
    "                sect_el, \"PAGE\", ID=f\"{code}.{sect_id:03}.{page_num:03}\"\n",
    "            )\n",
    "            \n",
    "            # Extract sentences from page\n",
    "            sentences = split_into_sentences(page_text)\n",
    "            \n",
    "            # Pair Chinese and Vietnamese sentences\n",
    "            pairs = pair_chinese_vietnamese_sentences(sentences)\n",
    "            \n",
    "            # Create STC elements\n",
    "            for sent_id, pair in enumerate(pairs, 1):\n",
    "                stc_el = ET.SubElement(\n",
    "                    page_el, \"STC\", ID=f\"{code}.{sect_id:03}.{page_num:03}.{sent_id:02}\"\n",
    "                )\n",
    "                \n",
    "                if pair[\"chinese\"] and pair[\"vietnamese\"]:\n",
    "                    # Both Chinese and Vietnamese - use C and V tags\n",
    "                    ET.SubElement(stc_el, \"C\").text = pair[\"chinese\"]\n",
    "                    ET.SubElement(stc_el, \"V\").text = pair[\"vietnamese\"]\n",
    "                elif pair[\"chinese\"]:\n",
    "                    # Only Chinese - use C tag\n",
    "                    ET.SubElement(stc_el, \"C\").text = pair[\"chinese\"]\n",
    "                elif pair[\"vietnamese\"]:\n",
    "                    stc_el.text = pair[\"vietnamese\"]  # Set text directly in STC as per user request\n",
    "                \n",
    "                total_pairs += 1\n",
    "    \n",
    "    # Step 5: Write XML\n",
    "    tree = ET.ElementTree(root)\n",
    "    write_pretty_xml(tree, output_path)\n",
    "    \n",
    "    print(f\"âœ… Created XML file: {output_path}\")\n",
    "    print(f\"ðŸ“Š Total sentence pairs: {total_pairs}\")\n",
    "    \n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "98d6c80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Processing PDF: /home/octoopt/workspace/projects/learn-from-basics/nlp-vietnamese-phd/temp/Trang-tu-nam-hoa-kinh.pdf\n",
      "ðŸ“„ Extracted 295 pages\n",
      "Found section 'Lá»œI NÃ“I Äáº¦U' on page 2\n",
      "Found section 'Lá»œI NÃ“I Äáº¦U' on page 8\n",
      "Found section 'TIá»‚U DáºªN' on page 11\n",
      "Found section 'Ná»˜I THIÃŠN' on page 57\n",
      "Found section 'TIÃŠU DIÃŠU DU' on page 58\n",
      "Found section 'TIÃŠU DIÃŠU DU' on page 60\n",
      "Found section 'Tá»€ Váº¬T LUáº¬N' on page 81\n",
      "Found section 'Tá»€ Váº¬T LUáº¬N' on page 85\n",
      "Found section 'DÆ¯á» NG SINH CHá»¦' on page 127\n",
      "Found section 'DÆ¯á» NG SINH CHá»¦' on page 128\n",
      "Found section 'Äá»¨C SUNG PHÃ™' on page 138\n",
      "Found section 'Äá»¨C SUNG PHÃ™' on page 140\n",
      "Found section 'Äáº I TÃ”NG SÆ¯' on page 157\n",
      "Found section 'á»¨NG Äáº¾ VÆ¯Æ NG' on page 183\n",
      "Found section 'NGOáº I THIÃŠN' on page 199\n",
      "Found section 'Lá»œI NÃ“I Äáº¦U' on page 200\n",
      "Found section 'Lá»œI NÃ“I Äáº¦U' on page 292\n",
      "Total sections found: 18\n",
      "- TIÃŠU DIÃŠU DU: 1 pages\n",
      "- Lá»œI NÃ“I Äáº¦U: 6 pages\n",
      "- Lá»œI NÃ“I Äáº¦U: 3 pages\n",
      "- TIá»‚U DáºªN: 46 pages\n",
      "- Ná»˜I THIÃŠN: 1 pages\n",
      "- TIÃŠU DIÃŠU DU: 2 pages\n",
      "- TIÃŠU DIÃŠU DU: 21 pages\n",
      "- Tá»€ Váº¬T LUáº¬N: 4 pages\n",
      "- Tá»€ Váº¬T LUáº¬N: 42 pages\n",
      "- DÆ¯á» NG SINH CHá»¦: 1 pages\n",
      "- DÆ¯á» NG SINH CHá»¦: 10 pages\n",
      "- Äá»¨C SUNG PHÃ™: 2 pages\n",
      "- Äá»¨C SUNG PHÃ™: 17 pages\n",
      "- Äáº I TÃ”NG SÆ¯: 26 pages\n",
      "- á»¨NG Äáº¾ VÆ¯Æ NG: 16 pages\n",
      "- NGOáº I THIÃŠN: 1 pages\n",
      "- Lá»œI NÃ“I Äáº¦U: 92 pages\n",
      "- Lá»œI NÃ“I Äáº¦U: 4 pages\n",
      "âœ… Created XML file: PAS_003_Trang-tu-nam-hoa-kinh.xml\n",
      "ðŸ“Š Total sentence pairs: 5553\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'PAS_003_Trang-tu-nam-hoa-kinh.xml'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_path = \"/home/octoopt/workspace/projects/learn-from-basics/nlp-vietnamese-phd/temp/Trang-tu-nam-hoa-kinh.pdf\"\n",
    "code = \"PAS_003\"\n",
    "des = f\"{code}_Trang-tu-nam-hoa-kinh.xml\"\n",
    "\n",
    "metadata = BOOK_METADATA[\"TRANG_TU_NAM_HOA_KINH_2\"]\n",
    "\n",
    "build_xml_for_book(pdf_path, metadata, des, code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3cf4e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sentences: 5548\n",
      "\n",
      "Number of sentences per section:\n",
      "- TIÃŠU DIÃŠU DU: 437 sentences\n",
      "- Lá»œI NÃ“I Äáº¦U: 2 sentences\n",
      "- TIá»‚U DáºªN: 893 sentences\n",
      "- Ná»˜I THIÃŠN: 0 sentences\n",
      "- Tá»€ Váº¬T LUáº¬N: 857 sentences\n",
      "- DÆ¯á» NG SINH CHá»¦: 232 sentences\n",
      "- Äá»¨C SUNG PHÃ™: 330 sentences\n",
      "- Äáº I TÃ”NG SÆ¯: 639 sentences\n",
      "- á»¨NG Äáº¾ VÆ¯Æ NG: 323 sentences\n",
      "- NGOáº I THIÃŠN: 0 sentences\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "\n",
    "def count_sentences(xml_file_path):\n",
    "    # Parse the XML file\n",
    "    tree = ET.parse(xml_file_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Find all STC elements (sentences)\n",
    "    sentences = root.findall(\".//STC\")\n",
    "\n",
    "    # Count the sentences\n",
    "    total_sentences = len(sentences)\n",
    "\n",
    "    # Count sentences per section\n",
    "    sections = {}\n",
    "    for section in root.findall(\".//SECT\"):\n",
    "        section_name = section.get(\"NAME\", \"Unknown\")\n",
    "        section_sentences = section.findall(\".//STC\")\n",
    "        sections[section_name] = len(section_sentences)\n",
    "\n",
    "    return total_sentences, sections\n",
    "\n",
    "\n",
    "# Path to your XML file\n",
    "xml_file = \"/home/octoopt/workspace/projects/learn-from-basics/nlp-vietnamese-phd/PAS_003_Trang-tu-nam-hoa-kinh.xml\"\n",
    "\n",
    "# Get the counts\n",
    "total, section_counts = count_sentences(xml_file)\n",
    "\n",
    "print(f\"Total number of sentences: {total}\")\n",
    "print(\"\\nNumber of sentences per section:\")\n",
    "for section, count in section_counts.items():\n",
    "    print(f\"- {section}: {count} sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "318fb0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document('/home/octoopt/workspace/projects/learn-from-basics/nlp-vietnamese-phd/temp/Nam-hoa-kinh.pdf')\n",
      "{'title': None, 'author': None, 'subject': None, 'keywords': None, 'creator': None, 'producer': 'iLovePDF', 'creation_date': None, 'mod_date': \"D:20221109160826+07'00'\", 'format': 'PDF 1.4', 'page_count': 139}\n"
     ]
    }
   ],
   "source": [
    "def get_pdf_metadata(pdf_path):\n",
    "    \"\"\"\n",
    "    Extract metadata from a PDF file using PyMuPDF.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing PDF metadata\n",
    "    \"\"\"\n",
    "    doc = pymupdf.open(pdf_path)\n",
    "    print(doc)\n",
    "    meta = doc.metadata\n",
    "\n",
    "    # Convert to a dictionary and clean up None values\n",
    "    metadata = {\n",
    "        \"title\": meta.get(\"title\", \"\").strip() or None,\n",
    "        \"author\": meta.get(\"author\", \"\").strip() or None,\n",
    "        \"subject\": meta.get(\"subject\", \"\").strip() or None,\n",
    "        \"keywords\": meta.get(\"keywords\", \"\").strip() or None,\n",
    "        \"creator\": meta.get(\"creator\", \"\").strip() or None,\n",
    "        \"producer\": meta.get(\"producer\", \"\").strip() or None,\n",
    "        \"creation_date\": meta.get(\"creationDate\", \"\").strip() or None,\n",
    "        \"mod_date\": meta.get(\"modDate\", \"\").strip() or None,\n",
    "        \"format\": meta.get(\"format\", \"\").strip() or None,\n",
    "        # 'encryption': meta.get('encryption', '\\n').strip() or None,\n",
    "        \"page_count\": len(doc),\n",
    "    }\n",
    "\n",
    "    doc.close()\n",
    "    return metadata\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "pdf_path = \"/home/octoopt/workspace/projects/learn-from-basics/nlp-vietnamese-phd/temp/Nam-hoa-kinh.pdf\"\n",
    "metadata = get_pdf_metadata(pdf_path)\n",
    "print(metadata)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
